---
title: "How do you apply PCA to Logistic Regression to remove Multicollinearity?"
author: "Mehmet Kadri Gofralılar"
date: "11/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(cowplot)
library(reshape2)
library(caret)
library(Amelia)
library(e1071)
library(tidyverse)
```


Multicollinearity occurs when features (input variables) are highly correlated with one or more of the other features in the dataset. It affects the performance of regression and classification models. PCA (Principal Component Analysis) takes advantage of multicollinearity and combines the highly correlated variables into a set of uncorrelated variables. Therefore, PCA can effectively eliminate multicollinearity between features.

In this post, we’ll build a logistic regression model on a classification dataset called breast_cancer data. The initial model can be considered as the base model. Then, we’ll apply PCA on breast_cancer data and build the logistic regression model again. After that, we’ll compare the performance between the base model and this model.

Let’s get started!

## Loading the breast_cancer dataset

```{r loading}
data <- read.csv("data/data.csv")
label <- select(data,diagnosis)
```


Now, let's create the heatmap to visualize correlation coefficients of continious variables and save it.


```{r heatmap}
cormat <- cor(subset(subset( data, select = - X ),select = - diagnosis))
melted_cormat <- melt(cormat)


ggplot(melted_cormat, aes(x=Var1, y=Var2, fill=value))+
geom_tile()+
scale_fill_gradientn(colours = c("black", "#5c0042", "#990159", "red", "#fee1b3", "#fef0be"), 
                     values = c(0,0.1,0.2,0.4,0.8,1), name = "")+
theme(axis.text.x=element_text(angle=90,hjust=0.5,vjust=0.5))+
labs(x = "", y = "", title = "Heatmap")



ggsave("heatmap.png",  width = 4000, height = 2500, units = "px")
```

As you can see in the heatmap, some of the features in the dataset are highly correlated with each other. So, there exists multicollinearity.

## Building a logistic regression model (Base model)

The following code block builds a logistic regression model on breast_cancer data.

```{r logistic_regression_model}
data <- subset( data, select = - id )
data <- subset( data, select = - X )

data$diagnosis <- as.factor(data$diagnosis)

############

set.seed(1)

partial_data <- createDataPartition(y=data$diagnosis,p=0.8,list = FALSE)

train_data <- data[partial_data,]
test_data <- data[-partial_data,]

model <- glm(diagnosis~.,data = train_data,family = "binomial")

############

prob <- predict(model , newdata = test_data)
pred <- ifelse(prob>0 , "M", "B")
mean(test_data$diagnosis == pred)
confusionMatrix(test_data$diagnosis,as.factor(pred))
```

As we can see, we have 92% accuracy rate. Let’s see whether we can improve the performance of the model by applying PCA.

## Applying PCA on breast_cancer data

First, let's apply PCA on our data and see what happens.

```{r PCA}
pca <- prcomp(data[3:31],center = TRUE, scale = TRUE)
summary(pca)
```

We can say that first 6 components have the capacity to capture 87% variability in the data. We’re interested to keep the first 6 components.

## Running PCA again with 6 components

It's time to use first 6 components and create a heatmap and save it.

```{r PCA_heatmap}
pca_for_heatmap <- as.data.frame(pca$x[,1:6])

pca_cormat <- cor(pca_for_heatmap)
meltec_pca_cormat <- melt(pca_cormat)

ggplot(meltec_pca_cormat, aes(x=Var1, y=Var2, fill=value))+
geom_tile()+
scale_fill_gradientn(colours = c("black", "#5c0042", "#990159", "red", "#fee1b3", "#fef0be"),values = c(0,0.1,0.2,0.4,0.8,1), name = "")+
theme(axis.text.x=element_text(angle=90,hjust=0.5,vjust=0.5))+
labs(x = "", y = "", title = "PCAHeatmap")

ggsave("PCA_heatmap.png",  width = 4000, height = 2500, units = "px")
```


## Building a logistic regression model on the transformed data

Since there is no correlation between components, we should see if there will be an improvement in our accuracy rate if we use logistic regression again.

```{r PCA_logistic_regression_model}

set.seed(5415)

pca_for_heatmap$diagnosis <- label$diagnosis

partial_pca_data <- createDataPartition(y=pca_for_heatmap$diagnosis,p=0.8,list = FALSE)

pca_train_data <- pca_for_heatmap[partial_pca_data,]
pca_test_data <- pca_for_heatmap[-partial_pca_data,]

pca_model <- glm(as.factor(diagnosis)~.,data = pca_train_data,family = "binomial")


pca_prob <- predict(pca_model , newdata = pca_test_data)
pca_pred <- ifelse(pca_prob>0 , "M", "B")
mean(pca_test_data$diagnosis == pca_pred)

```

## Conclusion

As we can see, the accuracy rate went up to 95% from 92%. In short, we can say that applying PCA can help us remove Multicollinearity while using Logistic Regression.
